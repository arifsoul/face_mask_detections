{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff5f2547",
   "metadata": {},
   "source": [
    "# Face Mask Detection (MLOps Enhanced)\n",
    "\n",
    "This notebook implements Face Mask Detection using **Faster R-CNN**, integrating \"Production-Grade\" practices:\n",
    "- **MLflow**: For experiment tracking (loss, metrics, artifacts).\n",
    "- **Mixed Precision (AMP)**: For faster training and lower memory usage.\n",
    "- **Advanced Evaluation**: Confusion Matrix and Classification Reports adapted for Object Detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e32c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from tqdm.auto import tqdm\n",
    "import shutil\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "from src.dataset import FaceMaskDataset\n",
    "from src.model import get_model\n",
    "from src.utils import convert_to_yolo_format\n",
    "import kagglehub\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Enable System Metrics Logging (CPU/GPU/RAM)\n",
    "try:\n",
    "    mlflow.enable_system_metrics_logging()\n",
    "except Exception as e:\n",
    "    print(f'Could not enable system metrics: {e}')\n",
    "\n",
    "# Configure MLflow Tracking URI\n",
    "root_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "db_path = os.path.join(root_dir, 'mlflow.db')\n",
    "tracking_uri = f'sqlite:///{db_path}'\n",
    "mlflow.set_tracking_uri(tracking_uri)\n",
    "print(f\"MLflow Tracking URI set to: {tracking_uri}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0136e61f",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "Downloading and setting up the Face Mask Dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5554d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Selection\n",
    "model_name = \"yolo26s\" # Options: fasterrcnn_mobilenet, fasterrcnn_resnet50, retinanet, yolo26s\n",
    "\n",
    "# Global Experiment Name (Early Definition)\n",
    "experiment_name = f\"FaceMask_{model_name.replace('.pt', '')}\"\n",
    "print(f\"Experiment Name: {experiment_name}\")\n",
    "\n",
    "# Dataset Download and Move\n",
    "try:\n",
    "    cache_path = kagglehub.dataset_download(\"andrewmvd/face-mask-detection\")\n",
    "    target_path = '../data'\n",
    "    \n",
    "    if not os.path.exists(os.path.join(target_path, 'images')):\n",
    "        print(f\"Moving data to {target_path}...\")\n",
    "        os.makedirs(target_path, exist_ok=True)\n",
    "        for item in os.listdir(cache_path):\n",
    "            s = os.path.join(cache_path, item)\n",
    "            d = os.path.join(target_path, item)\n",
    "            if os.path.isdir(s):\n",
    "                if os.path.exists(d): shutil.rmtree(d)\n",
    "                shutil.copytree(s, d)\n",
    "            else:\n",
    "                shutil.copy2(s, d)\n",
    "    ROOT_DIR = target_path\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    ROOT_DIR = '../data'\n",
    "\n",
    "print(f\"Dataset Root: {ROOT_DIR}\")\n",
    "\n",
    "# Transforms\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        # transforms.append(T.RandomHorizontalFlip(0.5)) # Optional augmentation\n",
    "        pass\n",
    "    return T.Compose(transforms)\n",
    "\n",
    "# Load Dataset\n",
    "dataset = FaceMaskDataset(ROOT_DIR, transforms=get_transform(train=False)) # We convert locally in loop if needed, or use separate\n",
    "# Ideally for training we want transforms, but for simplicity we keep it standard\n",
    "dataset_train_full = FaceMaskDataset(ROOT_DIR, transforms=get_transform(train=True))\n",
    "dataset_test_full = FaceMaskDataset(ROOT_DIR, transforms=get_transform(train=False))\n",
    "\n",
    "# Split\n",
    "torch.manual_seed(42)\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "test_split = int(0.2 * len(dataset))\n",
    "\n",
    "# Define Train/Test Indices\n",
    "train_indices = indices[:-test_split]\n",
    "test_indices = indices[-test_split:]\n",
    "\n",
    "print(f\"Total Images: {len(dataset)}\")\n",
    "print(f\"Train Split: {len(train_indices)}, Test Split: {len(test_indices)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1d7e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(\n",
    "    original_dataset,\n",
    "    subset_indices,\n",
    "    target_root,\n",
    "    target_classes=[1, 2, 3],\n",
    "    num_samples_per_class=200,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates a complete training dataset at target_root.\n",
    "    1. Copies ALL original images/xmls from subset_indices to target_root.\n",
    "    2. Generates additional synthetic data by SMART pasting objects onto backgrounds,\n",
    "       avoiding overlap with existing objects and preserving original annotations.\n",
    "    \"\"\"\n",
    "    from PIL import Image\n",
    "    img_dir = os.path.join(target_root, \"images\")\n",
    "    annot_dir = os.path.join(target_root, \"annotations\")\n",
    "    os.makedirs(img_dir, exist_ok=True)\n",
    "    os.makedirs(annot_dir, exist_ok=True)\n",
    "\n",
    "    root_imgs = original_dataset.img_dir\n",
    "    root_xmls = original_dataset.annot_dir\n",
    "\n",
    "    list_augmentations = [\n",
    "        T.RandomRotation(degrees=15, expand=True),\n",
    "        T.RandomHorizontalFlip(p=0.5),\n",
    "        T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),\n",
    "        T.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.8, 1.2), shear=10),\n",
    "    ]\n",
    "\n",
    "    # 1. Copy Original Subset (The Base)\n",
    "    print(f\"Copying {len(subset_indices)} original samples to {target_root}...\")\n",
    "    for idx in tqdm(subset_indices, desc=\"Copying Base Data\"):\n",
    "        xml_name = original_dataset.xmls[idx]\n",
    "        img_name = original_dataset.imgs[idx]\n",
    "        try:\n",
    "            shutil.copy2(\n",
    "                os.path.join(root_imgs, img_name), os.path.join(img_dir, img_name)\n",
    "            )\n",
    "            shutil.copy2(\n",
    "                os.path.join(root_xmls, xml_name), os.path.join(annot_dir, xml_name)\n",
    "            )\n",
    "        except FileNotFoundError:\n",
    "            pass  # Skip if files missing\n",
    "\n",
    "    # 2. Collect Objects for Augmentation\n",
    "    class_objects = {c: [] for c in target_classes}\n",
    "    backgrounds = []\n",
    "\n",
    "    print(\"Scanning dataset for source objects...\")\n",
    "    for idx in subset_indices:\n",
    "        xml_file = original_dataset.xmls[idx]\n",
    "        img_file = original_dataset.imgs[idx]\n",
    "        \n",
    "        # Filter Backgrounds: Must NOT contain non-target objects\n",
    "        has_non_target = False\n",
    "        try:\n",
    "            tree = ET.parse(os.path.join(root_xmls, xml_file))\n",
    "            root = tree.getroot()\n",
    "            \n",
    "            # Pre-scan for non-target objects\n",
    "            for obj in root.findall(\"object\"):\n",
    "                name = obj.find(\"name\").text\n",
    "                label = original_dataset.label_map.get(name, 0)\n",
    "                if label not in target_classes:\n",
    "                    has_non_target = True\n",
    "        \n",
    "            if not has_non_target:\n",
    "                backgrounds.append((img_file, xml_file))\n",
    "        \n",
    "            # Collect Source Objects\n",
    "            for obj in root.findall(\"object\"):\n",
    "                name = obj.find(\"name\").text\n",
    "                label = original_dataset.label_map.get(name, 0)\n",
    "                if label in target_classes:\n",
    "                    b = obj.find(\"bndbox\")\n",
    "                    box = [\n",
    "                        int(b.find(k).text) for k in [\"xmin\", \"ymin\", \"xmax\", \"ymax\"]\n",
    "                    ]\n",
    "                    class_objects[label].append(\n",
    "                        {\n",
    "                            \"img\": os.path.join(root_imgs, img_file),\n",
    "                            \"box\": box,\n",
    "                            \"name\": name,\n",
    "                        }\n",
    "                    )\n",
    "        except:\n",
    "            continue\n",
    "    def check_overlap(new_box, existing_boxes, threshold=0.1):\n",
    "        # new_box: [xmin, ymin, xmax, ymax]\n",
    "        nx1, ny1, nx2, ny2 = new_box\n",
    "        n_area = (nx2 - nx1) * (ny2 - ny1)\n",
    "\n",
    "        for box in existing_boxes:\n",
    "            ex1, ey1, ex2, ey2 = box\n",
    "            inter_x1 = max(nx1, ex1)\n",
    "            inter_y1 = max(ny1, ey1)\n",
    "            inter_x2 = min(nx2, ex2)\n",
    "            inter_y2 = min(ny2, ey2)\n",
    "\n",
    "            if inter_x2 > inter_x1 and inter_y2 > inter_y1:\n",
    "                inter_area = (inter_x2 - inter_x1) * (inter_y2 - inter_y1)\n",
    "                if inter_area > 0:\n",
    "                    # Check intersection ratio against the smaller box or union\n",
    "                    # Simply: if intersection > 10% of new box area, fail\n",
    "                    if inter_area / n_area > threshold:\n",
    "                        return True\n",
    "        return False\n",
    "\n",
    "    # 3. Generate New Synthetic Samples\n",
    "    total_new = 0\n",
    "    for cls in target_classes:\n",
    "        objs = class_objects[cls]\n",
    "        if not objs:\n",
    "            continue\n",
    "\n",
    "        print(f\"Generating {num_samples_per_class} new samples for Class {cls}...\")\n",
    "        count = 0\n",
    "        attempts = 0\n",
    "        max_attempts = num_samples_per_class * 10  # Safety limit\n",
    "\n",
    "        while count < num_samples_per_class and attempts < max_attempts:\n",
    "            attempts += 1\n",
    "\n",
    "            # Background\n",
    "            bg_img_name, bg_xml_name = random.choice(backgrounds)\n",
    "            bg_path = os.path.join(root_imgs, bg_img_name)\n",
    "            xml_bg_path = os.path.join(root_xmls, bg_xml_name)\n",
    "\n",
    "            bg = cv2.imread(bg_path)\n",
    "            if bg is None:\n",
    "                continue\n",
    "            h, w = bg.shape[:2]\n",
    "\n",
    "            # Get Existing Boxes\n",
    "            existing_boxes = []\n",
    "            original_objects = []  # To copy to new XML\n",
    "            try:\n",
    "                tree_bg = ET.parse(xml_bg_path)\n",
    "                root_bg = tree_bg.getroot()\n",
    "                for obj in root_bg.findall(\"object\"):\n",
    "                    original_objects.append(obj)  # Keep element to copy\n",
    "                    b = obj.find(\"bndbox\")\n",
    "                    existing_boxes.append(\n",
    "                        [int(b.find(k).text) for k in [\"xmin\", \"ymin\", \"xmax\", \"ymax\"]]\n",
    "                    )\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # Object to Paste\n",
    "            item = random.choice(objs)\n",
    "            src = cv2.imread(item[\"img\"])\n",
    "            if src is None:\n",
    "                continue\n",
    "            xmin, ymin, xmax, ymax = item[\"box\"]\n",
    "            crop = src[ymin:ymax, xmin:xmax]\n",
    "            if crop.size == 0:\n",
    "                continue\n",
    "\n",
    "            # --- SAFE AUGMENTATION ---\n",
    "            try:\n",
    "                crop_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
    "                pil_img = Image.fromarray(crop_rgb)\n",
    "\n",
    "                # Fix: Use random.choice and T.Compose correctly\n",
    "                aug_op = random.choice(list_augmentations)\n",
    "                aug = T.Compose([aug_op])\n",
    "                aug_img = aug(pil_img)\n",
    "\n",
    "                crop_aug = cv2.cvtColor(np.array(aug_img), cv2.COLOR_RGB2BGR)\n",
    "            except:\n",
    "                crop_aug = crop\n",
    "\n",
    "            # Resize (Random Scale)\n",
    "            scale = random.uniform(0.5, 1.2)\n",
    "            ch, cw = crop.shape[:2]\n",
    "            nw, nh = int(cw * scale), int(ch * scale)\n",
    "            nw = min(nw, w - 1)\n",
    "            nh = min(nh, h - 1)\n",
    "            if nw <= 0 or nh <= 0:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                crop = cv2.resize(crop_aug, (nw, nh))\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            # Find Valid Position (Try 10 times)\n",
    "            placed = False\n",
    "            for _ in range(10):\n",
    "                if w - nw <= 0 or h - nh <= 0:\n",
    "                    break\n",
    "                px = random.randint(0, w - nw)\n",
    "                py = random.randint(0, h - nh)\n",
    "                new_box = [px, py, px + nw, py + nh]\n",
    "\n",
    "                if not check_overlap(new_box, existing_boxes):\n",
    "                    # Paste\n",
    "                    bg[py : py + nh, px : px + nw] = crop\n",
    "                    placed = True\n",
    "                    break\n",
    "\n",
    "            if placed:\n",
    "                # Save\n",
    "                fname = f\"synth_{cls}_{total_new}.jpg\"\n",
    "                cv2.imwrite(os.path.join(img_dir, fname), bg)\n",
    "\n",
    "                # New XML\n",
    "                r = ET.Element(\"annotation\")\n",
    "                ET.SubElement(r, \"filename\").text = fname\n",
    "                sz = ET.SubElement(r, \"size\")\n",
    "                ET.SubElement(sz, \"width\").text = str(w)\n",
    "                ET.SubElement(sz, \"height\").text = str(h)\n",
    "\n",
    "                # Copy Original Objects\n",
    "                for obj in original_objects:\n",
    "                    r.append(obj)\n",
    "\n",
    "                # Add New Object\n",
    "                o = ET.SubElement(r, \"object\")\n",
    "                ET.SubElement(o, \"name\").text = item[\"name\"]\n",
    "                bb = ET.SubElement(o, \"bndbox\")\n",
    "                for k, v in zip([\"xmin\", \"ymin\", \"xmax\", \"ymax\"], new_box):\n",
    "                    ET.SubElement(bb, k).text = str(v)\n",
    "\n",
    "                ET.ElementTree(r).write(\n",
    "                    os.path.join(annot_dir, fname.replace(\".jpg\", \".xml\"))\n",
    "                )\n",
    "                count += 1\n",
    "                total_new += 1\n",
    "\n",
    "    print(f\"Total Generated (New Augmented w/ Collision Check): {total_new}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "synthetic_gen_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Synthetic Root\n",
    "synthetic_root = '../data/synthetic_augmented'\n",
    "\n",
    "# WORKFLOW: CLEANUP -> GENERATE\n",
    "# 1. Cleanup existing folder to avoid duplicates\n",
    "if os.path.exists(synthetic_root):\n",
    "    print(f\"Cleaning up existing synthetic data at {synthetic_root}...\")\n",
    "    try:\n",
    "        shutil.rmtree(synthetic_root)\n",
    "    except PermissionError:\n",
    "        print(\"Permission Error during cleanup. Retrying...\")\n",
    "        import time\n",
    "        time.sleep(1)\n",
    "        shutil.rmtree(synthetic_root, ignore_errors=True)\n",
    "\n",
    "# 2. Generate Synthetic Data\n",
    "# TARGET CLASSES: 2, 3\n",
    "print(\"Starting Synthetic Data Generation...\")\n",
    "generate_synthetic_data(dataset, train_indices, target_root=synthetic_root, \n",
    "                        target_classes=[2, 3], \n",
    "                        num_samples_per_class=400)\n",
    "print(\"Synthetic Data Generation Complete.\")\n",
    "\n",
    "# 3. Verify\n",
    "if os.path.exists(synthetic_root):\n",
    "    print(f\"Verified: {synthetic_root} created.\")\n",
    "else:\n",
    "    print(f\"Warning: {synthetic_root} was not created.\")\n",
    "\n",
    "# 4. Load Synthetic Dataset\n",
    "if os.path.exists(synthetic_root):\n",
    "    from src.dataset import FaceMaskDataset\n",
    "    # Re-import get_transform just in case or ensure it's available\n",
    "    # Assuming get_transform is defined earlier or imported\n",
    "    try:\n",
    "        synthetic_dataset = FaceMaskDataset(synthetic_root, transforms=get_transform(train=True))\n",
    "        print(f\"Successfully loaded synthetic dataset with {len(synthetic_dataset)} samples.\")\n",
    "    except NameError:\n",
    "        # Fallback if get_transform not in scope yet (though it should be)\n",
    "        print(\"Warning: get_transform not found. Defining dummy or relying on earlier cells.\")\n",
    "        # We assume dataset.py handles imports, but transform usually in notebook or utils\n",
    "        # Let's hope get_transform is globally defined in cell 2/3\n",
    "        synthetic_dataset = FaceMaskDataset(synthetic_root, transforms=None) \n",
    "else:\n",
    "    print(\"Synthetic root not found. Skipping dataset load.\")\n",
    "    synthetic_dataset = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "synthetic_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_class_distribution_comparison(original_dataset, train_indices, synthetic_dataset):\n",
    "    \"\"\"\n",
    "    Plots comparative class distribution between original (subset) and synthetic datasets.\n",
    "    \"\"\"\n",
    "    print(\"Calculating Comparative Class Distribution...\")\n",
    "    original_counts = {1: 0, 2: 0, 3: 0}\n",
    "    synthetic_counts = {1: 0, 2: 0, 3: 0}\n",
    "    label_names = {1: \"with_mask\", 2: \"without_mask\", 3: \"mask_weared_incorrect\"}\n",
    "    \n",
    "    # Count Original\n",
    "    if original_dataset and train_indices:\n",
    "        for idx in tqdm(train_indices, desc=\"Counting Original\"):\n",
    "            try:\n",
    "                xml_path = os.path.join(original_dataset.annot_dir, original_dataset.xmls[idx])\n",
    "                tree = ET.parse(xml_path)\n",
    "                for obj in tree.findall(\"object\"):\n",
    "                    name = obj.find(\"name\").text\n",
    "                    l = original_dataset.label_map.get(name, 0)\n",
    "                    if l in original_counts: original_counts[l] += 1\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # Count Synthetic (Now Full Dataset)\n",
    "    # Note: Synthetic dataset now contains COPIES of original too.\n",
    "    # So 'Synthetic Source' in plot will effectively show the Total.\n",
    "    # To make the plot 'Original vs Added Synthetic', we might need to subtract??\n",
    "    # BUT, the user said 'synthetic data lebih banyak' ...\n",
    "    # Let's just plot what is in synthetic folder as 'Synthetic' bar.\n",
    "    if synthetic_dataset:\n",
    "        for idx in range(len(synthetic_dataset)):\n",
    "            try:\n",
    "               xml_path = os.path.join(synthetic_dataset.annot_dir, synthetic_dataset.xmls[idx])\n",
    "               tree = ET.parse(xml_path)\n",
    "               for obj in tree.findall(\"object\"):\n",
    "                    name = obj.find(\"name\").text\n",
    "                    l = synthetic_dataset.label_map.get(name, 0)\n",
    "                    if l in synthetic_counts: synthetic_counts[l] += 1\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # Prepare Dataframe\n",
    "    import pandas as pd\n",
    "    data = []\n",
    "    for l in [1, 2, 3]:\n",
    "        name = label_names[l]\n",
    "        data.append({'Class': name, 'Count': original_counts[l], 'Source': 'Original'})\n",
    "        if synthetic_dataset:\n",
    "            data.append({'Class': name, 'Count': synthetic_counts[l], 'Source': 'Synthetic (Full)'})\n",
    "            \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = sns.barplot(data=df, x='Class', y='Count', hue='Source', palette='muted')\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container)\n",
    "    plt.title(\"Class Distribution: Original vs Synthetic (Full)\")\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.savefig(\"class_distribution_augmented.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Call the function\n",
    "if synthetic_dataset:\n",
    "    plot_class_distribution_comparison(dataset_train_full, train_indices, synthetic_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset_loader_reinit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Final Training Set\n",
    "if synthetic_dataset:\n",
    "    # Synthetic Dataset now includes the base original data\n",
    "    dataset_train = synthetic_dataset\n",
    "    print(f\"Training Data (Full Augmented): {len(dataset_train)}\")\n",
    "else:\n",
    "    dataset_train = torch.utils.data.Subset(dataset_train_full, train_indices)\n",
    "    print(\"No synthetic data generated. Using original training set.\")\n",
    "\n",
    "dataset_test = torch.utils.data.Subset(dataset_test_full, test_indices)\n",
    "\n",
    "# Dataloaders\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "num_epochs = 10 \n",
    "batch_size = 4\n",
    "num_workers = 0 \n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset_train, batch_size=batch_size, shuffle=True, num_workers=num_workers, collate_fn=collate_fn)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Final Train Size: {len(dataset_train)}, Test Size: {len(dataset_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yolo_conversion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLO Data Preparation\n",
    "# Using shared utility from src.utils\n",
    "if synthetic_dataset:\n",
    "    # Convert main dataset\n",
    "    print(f\"Converting dataset at {ROOT_DIR} to YOLO format...\")\n",
    "    yaml_path = convert_to_yolo_format(ROOT_DIR, dataset.label_map)\n",
    "    \n",
    "    # Convert synthetic dataset if exists\n",
    "    if os.path.exists(synthetic_root):\n",
    "        print(f\"Converting synthetic dataset at {synthetic_root} to YOLO format...\")\n",
    "        convert_to_yolo_format(synthetic_root, dataset.label_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca9ae08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Samples with Bounding Boxes (Matplotlib)\n",
    "\n",
    "def visualize_sample(dataset, idx):\n",
    "    img_tensor, target = dataset[idx]\n",
    "    img_np = img_tensor.permute(1, 2, 0).numpy()\n",
    "    # Assuming ToTensor (0-1), simple scaling could affect if normalization involved\n",
    "    # Usually we clamp just in case\n",
    "    img_np = np.clip(img_np, 0, 1)\n",
    "    \n",
    "    boxes = target['boxes'].tolist()\n",
    "    labels = target['labels'].tolist()\n",
    "    \n",
    "    label_names = {1: \"with_mask\", 2: \"without_mask\", 3: \"mask_weared_incorrect\"}\n",
    "    label_colors = {\n",
    "        1: 'green', \n",
    "        2: 'red',   \n",
    "        3: 'orange' \n",
    "    }\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "    ax.imshow(img_np)\n",
    "    ax.set_title(f\"Sample {idx}\")\n",
    "    ax.axis('off')\n",
    "    \n",
    "    for box, label in zip(boxes, labels):\n",
    "        xmin, ymin, xmax, ymax = box\n",
    "        width = xmax - xmin\n",
    "        height = ymax - ymin\n",
    "        \n",
    "        name = label_names.get(label, 'Unknown')\n",
    "        color = label_colors.get(label, 'white')\n",
    "        \n",
    "        rect = patches.Rectangle((xmin, ymin), width, height, linewidth=2, edgecolor=color, facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        ax.text(xmin, ymin-5, name, color='white', fontsize=10, backgroundcolor=color)\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "print(\"Visualizing 3 Random Samples...\")\n",
    "indices = random.sample(range(len(dataset)), 3)\n",
    "for idx in indices:\n",
    "    visualize_sample(dataset, idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f579370",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "num_classes = 4 # Background + 3 classes\n",
    "\n",
    "model = get_model(model_name, num_classes)\n",
    "# Model is now a Wrapper (YOLO or TorchVision) - Internalizing setup\n",
    "\n",
    "try:\n",
    "    model.to(device)\n",
    "except:\n",
    "    pass\n",
    "    model.to(device)\n",
    "    \n",
    "    # Optimizer & Scheduler (Only for PyTorch models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174daff1",
   "metadata": {},
   "source": [
    "## 2. Methodology: Training & Evaluation\n",
    "We adapt the reference notebook methods:\n",
    "- **`train_epoch`**: Uses `tqdm` for progress, `autocast` for AMP, and tracks Loss manually since Detection models return loss dicts.\n",
    "- **`evaluate`**: Since this is Object Detection, we compute metrics by matching predicted boxes to ground truth (IoU >= 0.5) and then calculating classification metrics (Confusion Matrix, Precision, Recall).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040f9e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_boxes(pred_boxes, true_boxes, iou_threshold=0.5):\n",
    "    if len(pred_boxes) == 0 or len(true_boxes) == 0:\n",
    "        return []\n",
    "    ious = torchvision.ops.box_iou(pred_boxes, true_boxes)\n",
    "    matches = []\n",
    "    ious = ious.clone()\n",
    "    for _ in range(len(pred_boxes)):\n",
    "        if ious.numel() == 0: break\n",
    "        val, idx = ious.flatten().max(0)\n",
    "        if val < iou_threshold: break\n",
    "        pred_idx = idx // ious.size(1)\n",
    "        true_idx = idx % ious.size(1)\n",
    "        matches.append((pred_idx.item(), true_idx.item()))\n",
    "        ious[pred_idx, :] = -1\n",
    "        ious[:, true_idx] = -1\n",
    "    return matches\n",
    "\n",
    "def evaluate(dataloader, device, model, epoch, class_names):\n",
    "    model.eval()\n",
    "    all_preds_cls = []\n",
    "    all_true_cls = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            images = list(image.to(device) for image in images)\n",
    "            outputs = model(images)\n",
    "            for i, output in enumerate(outputs):\n",
    "                target = targets[i]\n",
    "                true_boxes = target['boxes'].to(device)\n",
    "                true_labels = target['labels'].to(device)\n",
    "                pred_boxes = output['boxes']\n",
    "                pred_labels = output['labels']\n",
    "                pred_scores = output['scores']\n",
    "                keep = pred_scores > 0.5\n",
    "                pred_boxes = pred_boxes[keep]\n",
    "                pred_labels = pred_labels[keep]\n",
    "                matches = match_boxes(pred_boxes, true_boxes)\n",
    "                matched_pred_indices = set()\n",
    "                matched_true_indices = set()\n",
    "                for p_idx, t_idx in matches:\n",
    "                    all_preds_cls.append(pred_labels[p_idx].item())\n",
    "                    all_true_cls.append(true_labels[t_idx].item())\n",
    "                    matched_pred_indices.add(p_idx)\n",
    "                    matched_true_indices.add(t_idx)\n",
    "                for t_idx in range(len(true_labels)):\n",
    "                    if t_idx not in matched_true_indices:\n",
    "                        all_true_cls.append(true_labels[t_idx].item())\n",
    "                        all_preds_cls.append(0)\n",
    "                for p_idx in range(len(pred_labels)):\n",
    "                    if p_idx not in matched_pred_indices:\n",
    "                        all_true_cls.append(0)\n",
    "                        all_preds_cls.append(pred_labels[p_idx].item())\n",
    "\n",
    "    display_names = ['Background'] + list(class_names.values())\n",
    "    cm = confusion_matrix(all_true_cls, all_preds_cls, labels=[0, 1, 2, 3])\n",
    "    \n",
    "    # Seaborn Confusion Matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=display_names, yticklabels=display_names)\n",
    "    plt.title(f'Confusion Matrix (Epoch {epoch+1})')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    \n",
    "    cm_path = f\"confusion_matrix_epoch_{epoch+1}.png\"\n",
    "    plt.savefig(cm_path)\n",
    "    mlflow.log_artifact(cm_path)\n",
    "    plt.close()\n",
    "    \n",
    "    report = classification_report(all_true_cls, all_preds_cls, \n",
    "                                   labels=[0, 1, 2, 3], \n",
    "                                   target_names=display_names, \n",
    "                                   output_dict=True)\n",
    "    mlflow.log_metric(\"val_accuracy\", report['accuracy'], step=epoch)\n",
    "    mlflow.log_metric(\"val_f1_macro\", report['macro avg']['f1-score'], step=epoch)\n",
    "    \n",
    "    return report['accuracy']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6051c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Experiment Loop\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "\n",
    "with mlflow.start_run():\n",
    "    # Log params\n",
    "    mlflow.log_param(\"epochs\", num_epochs)\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    mlflow.log_param(\"model_architecture\", model_name)\n",
    "\n",
    "    # Prepare arguments for Unified Training\n",
    "    train_kwargs = {\n",
    "        \"epochs\": num_epochs,\n",
    "        \"device\": device,\n",
    "        \"project\": \"../mlruns\",\n",
    "        \"name\": experiment_name\n",
    "    }\n",
    "\n",
    "    if \"yolo\" in model_name:\n",
    "        print(\"Detected YOLO model. Training with MLflow tracking...\")\n",
    "        # Sync MLflow config for Ultralytics\n",
    "        os.environ[\"MLFLOW_TRACKING_URI\"] = mlflow.get_tracking_uri()\n",
    "        os.environ[\"MLFLOW_EXPERIMENT_NAME\"] = experiment_name\n",
    "\n",
    "        yaml_path = convert_to_yolo_format(ROOT_DIR, {})\n",
    "        mlflow.log_param(\"dataset_yaml\", yaml_path)\n",
    "        \n",
    "        # YOLO Specific Args\n",
    "        results = model.train(data=yaml_path, workers=0, **train_kwargs)\n",
    "\n",
    "        # Explicitly validate to get metrics and log them\n",
    "        print(\"Running validation to extract metrics...\")\n",
    "        metrics = model.val() # Uses best model\n",
    "        mlflow.log_metric(\"map50\", metrics.box.map50)\n",
    "        mlflow.log_metric(\"map50-95\", metrics.box.map)\n",
    "        mlflow.log_metric(\"precision\", metrics.box.mp)\n",
    "        mlflow.log_metric(\"recall\", metrics.box.mr)\n",
    "\n",
    "        # Cleanup Ghost Experiment (named after artifact path) if exists\n",
    "        try:\n",
    "            client = mlflow.tracking.MlflowClient()\n",
    "            ghost_exp = client.get_experiment_by_name(str(train_kwargs['project']))\n",
    "            if ghost_exp:\n",
    "                client.delete_experiment(ghost_exp.experiment_id)\n",
    "                print(f\"Cleaned up ghost experiment: {train_kwargs['project']}\")\n",
    "        except Exception as e:\n",
    "             pass\n",
    "\n",
    "    else:\n",
    "        # Standard PyTorch (Unified Wrapper)\n",
    "        print(f\"Detected TorchVision model: {model_name}. Training with Wrapper...\")\n",
    "        mlflow.log_param(\"framework\", \"PyTorch\")\n",
    "        \n",
    "        # Unified .train() call\n",
    "        results = model.train(train_dataloader, test_dataloader, **train_kwargs)\n",
    "\n",
    "    # Evaluation (Common)\n",
    "    # Note: TorchVision Wrapper handles training loop evaluation if implemented, or we do specific eval here\n",
    "    # For now, keeping the notebook simple.\n",
    "\n",
    "    # Enhanced MLflow Model Logging (Versioning + Input Example)\n",
    "    print(\"Logging model to MLflow Registry...\")\n",
    "    try:\n",
    "        if \"yolo\" not in model_name: # YOLO logs itself automatically usually, but RCNN doesn't\n",
    "             # Create input example\n",
    "             model.model.eval()\n",
    "             example_input, _ = dataset[0]\n",
    "             example_input = example_input.unsqueeze(0).to(device)\n",
    "             \n",
    "             mlflow.pytorch.log_model(\n",
    "                 pytorch_model=model.model, # Log the inner model\n",
    "                 artifact_path=\"model\",\n",
    "                 registered_model_name=experiment_name,\n",
    "                 input_example=example_input.cpu().numpy()\n",
    "             )\n",
    "    except Exception as e:\n",
    "        print(f\"MLflow Model Logging Failed: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
