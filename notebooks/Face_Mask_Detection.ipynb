{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff5f2547",
   "metadata": {},
   "source": [
    "# Face Mask Detection (MLOps Enhanced)\n",
    "\n",
    "This notebook implements Face Mask Detection using **Faster R-CNN**, integrating \"Production-Grade\" practices:\n",
    "- **MLflow**: For experiment tracking (loss, metrics, artifacts).\n",
    "- **Mixed Precision (AMP)**: For faster training and lower memory usage.\n",
    "- **Advanced Evaluation**: Confusion Matrix and Classification Reports adapted for Object Detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e32c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import math\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "from src.dataset import FaceMaskDataset\n",
    "from src.model import get_model\n",
    "from src.utils import convert_to_yolo_format\n",
    "import kagglehub\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Enable System Metrics Logging (CPU/GPU/RAM)\n",
    "try:\n",
    "    mlflow.enable_system_metrics_logging()\n",
    "except Exception as e:\n",
    "    print(f'Could not enable system metrics: {e}')\n",
    "\n",
    "# Configure MLflow Tracking URI to use a centralized SQLite DB in the root directory\n",
    "# This ensures 'mlflow ui' sees the same data regardless of where it's run\n",
    "root_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "db_path = os.path.join(root_dir, 'mlflow.db')\n",
    "tracking_uri = f'sqlite:///{db_path}'\n",
    "mlflow.set_tracking_uri(tracking_uri)\n",
    "print(f\"MLflow Tracking URI set to: {tracking_uri}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0136e61f",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "Downloading and setting up the Face Mask Dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5554d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Selection\n",
    "model_name = \"retinanet\" # Options: fasterrcnn_mobilenet, fasterrcnn_resnet50, retinanet, yolov8n\n",
    "\n",
    "# Global Experiment Name (Early Definition)\n",
    "experiment_name = f\"FaceMask_{model_name.replace('.pt', '')}\"\n",
    "print(f\"Experiment Name: {experiment_name}\")\n",
    "\n",
    "# Dataset Download and Move\n",
    "try:\n",
    "    cache_path = kagglehub.dataset_download(\"andrewmvd/face-mask-detection\")\n",
    "    target_path = '../data'\n",
    "    \n",
    "    if not os.path.exists(os.path.join(target_path, 'images')):\n",
    "        print(f\"Moving data to {target_path}...\")\n",
    "        os.makedirs(target_path, exist_ok=True)\n",
    "        for item in os.listdir(cache_path):\n",
    "            s = os.path.join(cache_path, item)\n",
    "            d = os.path.join(target_path, item)\n",
    "            if os.path.isdir(s):\n",
    "                if os.path.exists(d): shutil.rmtree(d)\n",
    "                shutil.copytree(s, d)\n",
    "            else:\n",
    "                shutil.copy2(s, d)\n",
    "    ROOT_DIR = target_path\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    ROOT_DIR = '../data'\n",
    "\n",
    "print(f\"Dataset Root: {ROOT_DIR}\")\n",
    "\n",
    "# Transforms\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        # transforms.append(T.RandomHorizontalFlip(0.5)) # Optional augmentation\n",
    "        pass\n",
    "    return T.Compose(transforms)\n",
    "\n",
    "# Load Dataset\n",
    "dataset = FaceMaskDataset(ROOT_DIR, transforms=get_transform(train=False)) # We convert locally in loop if needed, or use separate\n",
    "# Ideally for training we want transforms, but for simplicity we keep it standard\n",
    "dataset_train_full = FaceMaskDataset(ROOT_DIR, transforms=get_transform(train=True))\n",
    "dataset_test_full = FaceMaskDataset(ROOT_DIR, transforms=get_transform(train=False))\n",
    "\n",
    "# Split\n",
    "torch.manual_seed(42)\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "test_split = int(0.1 * len(dataset))\n",
    "dataset_train = torch.utils.data.Subset(dataset_train_full, indices[:-test_split])\n",
    "dataset_test = torch.utils.data.Subset(dataset_test_full, indices[-test_split:])\n",
    "\n",
    "# Dataloaders\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "num_epochs = 50 # Default epochs\n",
    "batch_size = 4\n",
    "num_workers = 0 # Windows safe\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset_train, batch_size=batch_size, shuffle=True, num_workers=num_workers, collate_fn=collate_fn)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Train Size: {len(dataset_train)}, Test Size: {len(dataset_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb32ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we target the correct experiment\n",
    "try:\n",
    "    # Ensure root path for artifacts\n",
    "    root_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "    artifact_path = os.path.join(root_dir, 'mlruns')\n",
    "    mlflow.create_experiment(experiment_name, artifact_location=f'file:///{artifact_path.replace(os.sep, \"/\")}')\n",
    "except:\n",
    "    pass\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "print(\"Analyzing Class Distribution...\")\n",
    "with mlflow.start_run(run_name=\"Data_Distribution_Log\"):\n",
    "    # Visualize Class Distribution (Plotly)\n",
    "    label_counts = {1: 0, 2: 0, 3: 0}\n",
    "    label_names = {1: \"with_mask\", 2: \"without_mask\", 3: \"mask_weared_incorrectly\"}\n",
    "    \n",
    "    # Iterate to count labels\n",
    "    for _, target in tqdm(dataset, desc=\"Counting Classes\"):\n",
    "        labels = target['labels'].tolist()\n",
    "        for l in labels:\n",
    "            if l in label_counts:\n",
    "                label_counts[l] += 1\n",
    "    \n",
    "    counts = [label_counts[k] for k in sorted(label_counts.keys())]\n",
    "    names = [label_names[k] for k in sorted(label_counts.keys())]\n",
    "    \n",
    "    fig = px.bar(\n",
    "        x=names, \n",
    "        y=counts, \n",
    "        title=\"Class Distribution\", \n",
    "        labels={'x': 'Class', 'y': 'Count'}, \n",
    "        color=names,\n",
    "        text=counts\n",
    "    )\n",
    "    fig.update_traces(textposition='outside')\n",
    "    fig.write_html(\"class_distribution.html\")\n",
    "    mlflow.log_artifact(\"class_distribution.html\")\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca9ae08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Random Samples with Bounding Boxes (Plotly)\n",
    "import random\n",
    "\n",
    "def visualize_sample(dataset, idx):\n",
    "    img_tensor, target = dataset[idx]\n",
    "    # Convert tensor [C, H, W] -> [H, W, C]\n",
    "    img_np = img_tensor.permute(1, 2, 0).numpy()\n",
    "    # Simple Denormalize/Scale (assuming standard ToTensor 0-1)\n",
    "    img_np = (img_np * 255).astype(np.uint8)\n",
    "    \n",
    "    boxes = target['boxes'].tolist()\n",
    "    labels = target['labels'].tolist()\n",
    "    \n",
    "    label_names = {1: \"with_mask\", 2: \"without_mask\", 3: \"mask_weared_incorrectly\"}\n",
    "    label_colors = {\n",
    "        1: 'green', # with_mask\n",
    "        2: 'red',   # without_mask\n",
    "        3: 'orange' # incorrect\n",
    "    }\n",
    "    \n",
    "    fig = px.imshow(img_np)\n",
    "    \n",
    "    for box, label in zip(boxes, labels):\n",
    "        xmin, ymin, xmax, ymax = box\n",
    "        name = label_names.get(label, 'Unknown')\n",
    "        color = label_colors.get(label, 'white')\n",
    "        \n",
    "        fig.add_shape(\n",
    "            type=\"rect\",\n",
    "            x0=xmin, y0=ymin, x1=xmax, y1=ymax,\n",
    "            line=dict(color=color, width=3),\n",
    "        )\n",
    "        # Add text annotation near box\n",
    "        fig.add_annotation(\n",
    "            x=xmin, y=ymin,\n",
    "            text=name,\n",
    "            showarrow=False,\n",
    "            yshift=10,\n",
    "            xanchor='left',\n",
    "            bgcolor=color,\n",
    "            font=dict(color='white')\n",
    "        )\n",
    "        \n",
    "    fig.update_layout(title=f\"Sample {idx} (Green=Mask, Red=No Mask, Orange=Incorrect)\")\n",
    "    fig.show()\n",
    "\n",
    "print(\"Visualizing 3 Random Samples...\")\n",
    "indices = random.sample(range(len(dataset)), 3)\n",
    "for idx in indices:\n",
    "    visualize_sample(dataset, idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f579370",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "num_classes = 4 # Background + 3 classes\n",
    "\n",
    "model = get_model(model_name, num_classes)\n",
    "# Model is now a Wrapper (YOLO or TorchVision) - Internalizing setup\n",
    "\n",
    "try:\n",
    "    model.to(device)\n",
    "except:\n",
    "    pass\n",
    "    model.to(device)\n",
    "    \n",
    "    # Optimizer & Scheduler (Only for PyTorch models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174daff1",
   "metadata": {},
   "source": [
    "## 2. Methodology: Training & Evaluation\n",
    "We adapt the reference notebook methods:\n",
    "- **`train_epoch`**: Uses `tqdm` for progress, `autocast` for AMP, and tracks Loss manually since Detection models return loss dicts.\n",
    "- **`evaluate`**: Since this is Object Detection, we compute metrics by matching predicted boxes to ground truth (IoU >= 0.5) and then calculating classification metrics (Confusion Matrix, Precision, Recall).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040f9e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_boxes(pred_boxes, true_boxes, iou_threshold=0.5):\n",
    "    # Simple IoU matching\n",
    "    if len(pred_boxes) == 0 or len(true_boxes) == 0:\n",
    "        return []\n",
    "        \n",
    "    ious = torchvision.ops.box_iou(pred_boxes, true_boxes)\n",
    "    matches = []\n",
    "    \n",
    "    # Greedy matching\n",
    "    for i in range(len(pred_boxes)):\n",
    "        best_iou, best_idx = ious[i].max(dim=0)\n",
    "        if best_iou > iou_threshold:\n",
    "            matches.append((i, best_idx.item()))\n",
    "            # mask out to prevent reuse? Simple greedy doesn't need complex mask for basic stats\n",
    "            ious[:, best_idx] = -1 \n",
    "            \n",
    "    return matches\n",
    "\n",
    "def evaluate(dataloader, device, model, epoch, class_names):\n",
    "    model.eval()\n",
    "    all_preds_cls = []\n",
    "    all_true_cls = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            images = list(image.to(device) for image in images)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            for i, output in enumerate(outputs):\n",
    "                target = targets[i]\n",
    "                \n",
    "                true_boxes = target['boxes'].to(device)\n",
    "                true_labels = target['labels'].to(device)\n",
    "                \n",
    "                pred_boxes = output['boxes']\n",
    "                pred_labels = output['labels']\n",
    "                pred_scores = output['scores']\n",
    "                \n",
    "                # Filter low confidence\n",
    "                keep = pred_scores > 0.5\n",
    "                pred_boxes = pred_boxes[keep]\n",
    "                pred_labels = pred_labels[keep]\n",
    "                \n",
    "                matches = match_boxes(pred_boxes, true_boxes)\n",
    "                \n",
    "                matched_pred_indices = set()\n",
    "                matched_true_indices = set()\n",
    "                \n",
    "                for p_idx, t_idx in matches:\n",
    "                    all_preds_cls.append(pred_labels[p_idx].item())\n",
    "                    all_true_cls.append(true_labels[t_idx].item())\n",
    "                    matched_pred_indices.add(p_idx)\n",
    "                    matched_true_indices.add(t_idx)\n",
    "                \n",
    "                # False Negatives (Missed ground truths)\n",
    "                for t_idx in range(len(true_labels)):\n",
    "                    if t_idx not in matched_true_indices:\n",
    "                        all_true_cls.append(true_labels[t_idx].item())\n",
    "                        all_preds_cls.append(0) # 0 is background/missed\n",
    "                        \n",
    "                # False Positives (Spurious detections)\n",
    "                for p_idx in range(len(pred_labels)):\n",
    "                    if p_idx not in matched_pred_indices:\n",
    "                        all_true_cls.append(0) # Background\n",
    "                        all_preds_cls.append(pred_labels[p_idx].item())\n",
    "\n",
    "    # Metrics\n",
    "    # We add 'Background' to class names for display if not present\n",
    "    display_names = ['Background'] + list(class_names.values())\n",
    "    \n",
    "    # Filter labels to be within range\n",
    "    unique_labels = sorted(list(set(all_true_cls) | set(all_preds_cls)))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(all_true_cls, all_preds_cls, labels=[0, 1, 2, 3])\n",
    "    \n",
    "    # Plotly Confusion Matrix\n",
    "    # Reverse formatting to match heatmap expectation (y=True, x=Predicted)\n",
    "    # But annotating heatmap usually takes z, x, y\n",
    "    \n",
    "    # z = cm\n",
    "    # x = display_names (Predicted)\n",
    "    # y = display_names (True)\n",
    "    \n",
    "    # Re-order logic if needed, but standard cm is [True, Pred]\n",
    "    \n",
    "    fig = ff.create_annotated_heatmap(\n",
    "        z=cm,\n",
    "        x=display_names,\n",
    "        y=display_names,\n",
    "        colorscale='Blues',\n",
    "        showscale=True\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        title=f'Confusion Matrix (Epoch {epoch+1})',\n",
    "        xaxis_title='Predicted',\n",
    "        yaxis_title='True Label'\n",
    "    )\n",
    "    \n",
    "    cm_html_path = f\"confusion_matrix_epoch_{epoch+1}.html\"\n",
    "    fig.write_html(cm_html_path)\n",
    "    mlflow.log_artifact(cm_html_path)\n",
    "    # fig.show() # Optional inside notebook\n",
    "    \n",
    "    report = classification_report(all_true_cls, all_preds_cls, labels=[0, 1, 2, 3], target_names=display_names, output_dict=True)\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"val_accuracy\", report['accuracy'], step=epoch)\n",
    "    mlflow.log_metric(\"val_f1_macro\", report['macro avg']['f1-score'], step=epoch)\n",
    "    \n",
    "    return report['accuracy']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6051c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Experiment Loop\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "\n",
    "with mlflow.start_run():\n",
    "    # Log params\n",
    "    mlflow.log_param(\"epochs\", num_epochs)\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    mlflow.log_param(\"model_architecture\", model_name)\n",
    "\n",
    "    # Prepare arguments for Unified Training\n",
    "    train_kwargs = {\n",
    "        \"epochs\": num_epochs,\n",
    "        \"device\": device,\n",
    "        \"project\": \"../mlruns\",\n",
    "        \"name\": experiment_name\n",
    "    }\n",
    "\n",
    "    if \"yolo\" in model_name:\n",
    "        print(\"Detected YOLO model. Training with MLflow tracking...\")\n",
    "        # Sync MLflow config for Ultralytics\n",
    "        os.environ[\"MLFLOW_TRACKING_URI\"] = mlflow.get_tracking_uri()\n",
    "        os.environ[\"MLFLOW_EXPERIMENT_NAME\"] = experiment_name\n",
    "\n",
    "        yaml_path = convert_to_yolo_format(ROOT_DIR, {})\n",
    "        mlflow.log_param(\"dataset_yaml\", yaml_path)\n",
    "        \n",
    "        # YOLO Specific Args\n",
    "        results = model.train(data=yaml_path, workers=0, **train_kwargs)\n",
    "\n",
    "        # Explicitly validate to get metrics and log them\n",
    "        print(\"Running validation to extract metrics...\")\n",
    "        metrics = model.val() # Uses best model\n",
    "        mlflow.log_metric(\"map50\", metrics.box.map50)\n",
    "        mlflow.log_metric(\"map50-95\", metrics.box.map)\n",
    "        mlflow.log_metric(\"precision\", metrics.box.mp)\n",
    "        mlflow.log_metric(\"recall\", metrics.box.mr)\n",
    "\n",
    "        # Cleanup Ghost Experiment (named after artifact path) if exists\n",
    "        try:\n",
    "            client = mlflow.tracking.MlflowClient()\n",
    "            ghost_exp = client.get_experiment_by_name(str(train_kwargs['project']))\n",
    "            if ghost_exp:\n",
    "                client.delete_experiment(ghost_exp.experiment_id)\n",
    "                print(f\"Cleaned up ghost experiment: {train_kwargs['project']}\")\n",
    "        except Exception as e:\n",
    "             pass\n",
    "\n",
    "    else:\n",
    "        # Standard PyTorch (Unified Wrapper)\n",
    "        print(f\"Detected TorchVision model: {model_name}. Training with Wrapper...\")\n",
    "        mlflow.log_param(\"framework\", \"PyTorch\")\n",
    "        \n",
    "        # Unified .train() call\n",
    "        results = model.train(train_dataloader, test_dataloader, **train_kwargs)\n",
    "\n",
    "    # Evaluation (Common)\n",
    "    # Note: TorchVision Wrapper handles training loop evaluation if implemented, or we do specific eval here\n",
    "    # For now, keeping the notebook simple.\n",
    "\n",
    "    # Enhanced MLflow Model Logging (Versioning + Input Example)\n",
    "    print(\"Logging model to MLflow Registry...\")\n",
    "    try:\n",
    "        if \"yolo\" not in model_name: # YOLO logs itself automatically usually, but RCNN doesn't\n",
    "             # Create input example\n",
    "             model.model.eval()\n",
    "             example_input, _ = dataset[0]\n",
    "             example_input = example_input.unsqueeze(0).to(device)\n",
    "             \n",
    "             mlflow.pytorch.log_model(\n",
    "                 pytorch_model=model.model, # Log the inner model\n",
    "                 artifact_path=\"model\",\n",
    "                 registered_model_name=experiment_name,\n",
    "                 input_example=example_input.cpu().numpy()\n",
    "             )\n",
    "    except Exception as e:\n",
    "        print(f\"MLflow Model Logging Failed: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
